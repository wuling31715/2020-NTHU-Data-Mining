{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/Danny/Data-Mining/lab2/kaggle/data/'\n",
    "model_path = '/home/Danny/pytorch/model/'\n",
    "source_folder = '/home/Danny/Data-Mining/lab2/kaggle/data'\n",
    "destination_folder = '/home/Danny/pytorch/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x344d1b</td>\n",
       "      <td>@MichaelSpathITH Oh man. I finally get to list...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x34d97c</td>\n",
       "      <td>Why order early if you are last to receive? @P...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x388ad7</td>\n",
       "      <td>It's #impossible.  To express how much I &lt;LH&gt; ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x353a70</td>\n",
       "      <td>@POTUS Before you speak or tweet, please learn...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x37f73c</td>\n",
       "      <td>3 day Social Media Detox is ended ðŸ™‹ðŸ˜‡ I feel re...</td>\n",
       "      <td>trust</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049097</th>\n",
       "      <td>0x23c02e</td>\n",
       "      <td>Settling in for #mountainmen and then &lt;LH&gt; two...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049098</th>\n",
       "      <td>0x2de236</td>\n",
       "      <td>False Doctrine of Infallibility: If I find any...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049099</th>\n",
       "      <td>0x25cc57</td>\n",
       "      <td>#-#EGYPTIAN-#SYMBOLS-#DESIGN-#BLACK-#GOLD-#CAS...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049100</th>\n",
       "      <td>0x36b83c</td>\n",
       "      <td>#Goodnight everyone. May 2morrow be more lovin...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049101</th>\n",
       "      <td>0x33db76</td>\n",
       "      <td>Closed Sell 2.1 Lots EURUSD 1.17343 for +10.7 ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1049102 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x344d1b  @MichaelSpathITH Oh man. I finally get to list...   \n",
       "1        0x34d97c  Why order early if you are last to receive? @P...   \n",
       "2        0x388ad7  It's #impossible.  To express how much I <LH> ...   \n",
       "3        0x353a70  @POTUS Before you speak or tweet, please learn...   \n",
       "4        0x37f73c  3 day Social Media Detox is ended ðŸ™‹ðŸ˜‡ I feel re...   \n",
       "...           ...                                                ...   \n",
       "1049097  0x23c02e  Settling in for #mountainmen and then <LH> two...   \n",
       "1049098  0x2de236  False Doctrine of Infallibility: If I find any...   \n",
       "1049099  0x25cc57  #-#EGYPTIAN-#SYMBOLS-#DESIGN-#BLACK-#GOLD-#CAS...   \n",
       "1049100  0x36b83c  #Goodnight everyone. May 2morrow be more lovin...   \n",
       "1049101  0x33db76  Closed Sell 2.1 Lots EURUSD 1.17343 for +10.7 ...   \n",
       "\n",
       "              emotion  label  \n",
       "0        anticipation    1.0  \n",
       "1             sadness    5.0  \n",
       "2                 joy    4.0  \n",
       "3                 joy    4.0  \n",
       "4               trust    7.0  \n",
       "...               ...    ...  \n",
       "1049097       sadness    5.0  \n",
       "1049098  anticipation    1.0  \n",
       "1049099  anticipation    1.0  \n",
       "1049100           joy    4.0  \n",
       "1049101           joy    4.0  \n",
       "\n",
       "[1049102 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv((data_path + 'train.csv'), encoding='utf8', engine='python')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x2c5570</td>\n",
       "      <td>POST-SURGURY &lt;LH&gt; TWP: 68 YOF - 498 &lt;LH&gt; #BLAI...</td>\n",
       "      <td>trust</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2a39c7</td>\n",
       "      <td>@XavierDLeau @Blike_Dante @donnabrazile you al...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x252f67</td>\n",
       "      <td>We all know @RealDonalTrump is not bright. He ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2d569e</td>\n",
       "      <td>@followFALO We would love to collaborate with ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x238f8d</td>\n",
       "      <td>Love is amazing and &lt;LH&gt; is amazing.  I just &lt;...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116576</th>\n",
       "      <td>0x230218</td>\n",
       "      <td>He made me love Aslan before I knew who Aslan ...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116577</th>\n",
       "      <td>0x1fead9</td>\n",
       "      <td>&lt;LH&gt; that two of the places that I most freque...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116578</th>\n",
       "      <td>0x356d24</td>\n",
       "      <td>Hahah, Rooney!  Only goes and cups his ears af...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116579</th>\n",
       "      <td>0x1ee4d4</td>\n",
       "      <td>@TrumpTheHill &lt;LH&gt; and continued fear. Thank @...</td>\n",
       "      <td>fear</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116580</th>\n",
       "      <td>0x2ade94</td>\n",
       "      <td>I can't wait for match of @ShaneMcMahon and @F...</td>\n",
       "      <td>joy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116581 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id                                               text  \\\n",
       "0       0x2c5570  POST-SURGURY <LH> TWP: 68 YOF - 498 <LH> #BLAI...   \n",
       "1       0x2a39c7  @XavierDLeau @Blike_Dante @donnabrazile you al...   \n",
       "2       0x252f67  We all know @RealDonalTrump is not bright. He ...   \n",
       "3       0x2d569e  @followFALO We would love to collaborate with ...   \n",
       "4       0x238f8d  Love is amazing and <LH> is amazing.  I just <...   \n",
       "...          ...                                                ...   \n",
       "116576  0x230218  He made me love Aslan before I knew who Aslan ...   \n",
       "116577  0x1fead9  <LH> that two of the places that I most freque...   \n",
       "116578  0x356d24  Hahah, Rooney!  Only goes and cups his ears af...   \n",
       "116579  0x1ee4d4  @TrumpTheHill <LH> and continued fear. Thank @...   \n",
       "116580  0x2ade94  I can't wait for match of @ShaneMcMahon and @F...   \n",
       "\n",
       "             emotion  label  \n",
       "0              trust    7.0  \n",
       "1            disgust    2.0  \n",
       "2            sadness    5.0  \n",
       "3                joy    4.0  \n",
       "4                joy    4.0  \n",
       "...              ...    ...  \n",
       "116576  anticipation    1.0  \n",
       "116577           joy    4.0  \n",
       "116578         anger    0.0  \n",
       "116579          fear    3.0  \n",
       "116580           joy    4.0  \n",
       "\n",
       "[116581 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = pd.read_csv((data_path + 'valid.csv'), encoding='utf8', engine='python')\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        os.system('nvidia-smi -q -d Memory | grep -A4 GPU| grep Free > gpu_memory')\n",
    "        with open('gpu_memory', 'r') as f:\n",
    "            print(f.read())            \n",
    "        memory_available_list = [int(x.split()[2]) for x in open('gpu_memory', 'r').readlines()]\n",
    "        free_gpu_id = int(np.argmax(memory_available_list))\n",
    "        print(free_gpu_id)\n",
    "        return 'cuda:{}'.format(free_gpu_id)\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Free                        : 11008 MiB\n",
      "        Free                        : 11005 MiB\n",
      "        Free                        : 1601 MiB\n",
      "\n",
      "0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(get_gpu())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Danny/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 48\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, \n",
    "                    use_vocab=False, \n",
    "                    batch_first=True, \n",
    "                    dtype=torch.float)\n",
    "\n",
    "text_field = Field(use_vocab=False, \n",
    "                   tokenize=tokenizer.encode, \n",
    "                   lower=False, \n",
    "                   include_lengths=False, \n",
    "                   batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, \n",
    "                   pad_token=PAD_INDEX, \n",
    "                   unk_token=UNK_INDEX)\n",
    "\n",
    "fields = [('tweet_id', None),\n",
    "          ('text', text_field),\n",
    "          ('emotion', None),\n",
    "          ('label', label_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Danny/anaconda3/lib/python3.6/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/Danny/anaconda3/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 28s, sys: 754 ms, total: 8min 29s\n",
      "Wall time: 8min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, \n",
    "                                           train='train.csv', \n",
    "                                           validation='valid.csv',\n",
    "                                           test='test.csv', \n",
    "                                           format='CSV', \n",
    "                                           fields=fields, \n",
    "                                           skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [101, 1030, 17784, 15069, 8939, 2821, 2158, 1012, 1045, 2633, 2131, 2000, 4952, 2000, 1996, 2265, 2023, 2733, 999, 999, 999, 1026, 1048, 2232, 1028, 102], 'label': '1'}\n",
      "{'text': [101, 2695, 1011, 7505, 27390, 2100, 1026, 1048, 2232, 1028, 1056, 2860, 2361, 1024, 6273, 10930, 2546, 1011, 4749, 2620, 1026, 1048, 2232, 1028, 1001, 10503, 13731, 16428, 102], 'label': '7'}\n",
      "{'text': [101, 2042, 1037, 1001, 2733, 2085, 1001, 2144, 1045, 1026, 1048, 2232, 1028, 2026, 1001, 3566, 1012, 1045, 1001, 3335, 2014, 1012, 1001, 2016, 2003, 1013, 2001, 1013, 1001, 5091, 1001, 2097, 4783, 2026, 1001, 2190, 19699, 9013, 2094, 1012, 1001, 2941, 4887, 16774, 2594, 102], 'label': '6'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train[0]))\n",
    "print(vars(valid[0]))\n",
    "print(vars(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterators\n",
    "\n",
    "train_iter = BucketIterator(train, \n",
    "                            batch_size=batch_size, \n",
    "#                             sort_key=lambda x: len(x.text),\n",
    "                            device=device, \n",
    "                            train=True, \n",
    "                            sort=False,\n",
    "                            sort_within_batch=True)\n",
    "\n",
    "valid_iter = BucketIterator(valid, \n",
    "                            batch_size=batch_size, \n",
    "#                             sort_key=lambda x: len(x.text),\n",
    "                            device=device, \n",
    "                            train=True, \n",
    "                            sort=False,\n",
    "                            sort_within_batch=True)\n",
    "\n",
    "test_iter = Iterator(test, \n",
    "                     batch_size=batch_size, \n",
    "                     device=device, \n",
    "                     train=False, \n",
    "                     shuffle=False, \n",
    "                     sort=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train_model(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = num_epochs,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for (labels, title, text, titletext), _ in train_loader:\n",
    "            labels = labels.type(torch.LongTensor)           \n",
    "            labels = labels.to(device)\n",
    "            titletext = titletext.type(torch.LongTensor)  \n",
    "            titletext = titletext.to(device)\n",
    "            output = model(titletext, labels)\n",
    "            loss, _ = output\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                    # validation loop\n",
    "                    for (labels, title, text, titletext), _ in valid_loader:\n",
    "                        labels = labels.type(torch.LongTensor)           \n",
    "                        labels = labels.to(device)\n",
    "                        titletext = titletext.type(torch.LongTensor)  \n",
    "                        titletext = titletext.to(device)\n",
    "                        output = model(titletext, labels)\n",
    "                        loss, _ = output\n",
    "                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "                end = time.time()\n",
    "                second = end - start\n",
    "                if second > 60:\n",
    "                    minute = second // 60\n",
    "                    duration = '{:.4f} m'.format(minute)\n",
    "                else:\n",
    "                    duration = '{:.4f} s'.format(second)\n",
    "                \n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}, Time: {}'\n",
    "                      .format(epoch+1, \n",
    "                              num_epochs, \n",
    "                              global_step, \n",
    "                              num_epochs*len(train_loader),\n",
    "                              average_train_loss, \n",
    "                              average_valid_loss, \n",
    "                              duration,\n",
    "                             ))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = BERT().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "train_model(model=model, optimizer=optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
