{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "model_name = 'BiLSTM_BERT_tiny'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/Danny/Data-Mining/lab2/kaggle/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = list()\n",
    "with open(data_path + 'tweets_DM.json' , 'r') as file:\n",
    "    for line in file:\n",
    "        json_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = list()\n",
    "for json in json_list:\n",
    "    tweet_id = json['_source']['tweet']['tweet_id']\n",
    "    hashtags = json['_source']['tweet']['hashtags']\n",
    "    hashtag = ' '.join(hashtags)\n",
    "    text = json['_source']['tweet']['text']\n",
    "    text = text + ' ' + hashtag\n",
    "    tweet_list.append([tweet_id, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(tweet_list, columns=['tweet_id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = pd.read_csv(data_path + 'emotion.csv')\n",
    "# emotion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification_df = pd.read_csv(data_path + 'data_identification.csv')\n",
    "# identification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = identification_df[identification_df['identification'] == 'test']\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = text_df.merge(emotion_df, left_on='tweet_id', right_on='tweet_id')\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = train_df['text'].to_list()\n",
    "y_list = train_df['emotion'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455563"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list_list = list()\n",
    "for x in x_list:\n",
    "    x_list_list.append(x.split())\n",
    "len(x_list_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931560 931560\n",
      "291113 291113\n",
      "232890 232890\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_list_list, y_list, test_size=0.2, random_state=42)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "print(len(train_x), len(train_y))\n",
    "print(len(test_x), len(test_y))\n",
    "print(len(valid_x), len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-30 17:09:04,989 [DEBUG] kashgari - ------------------------------------------------\n",
      "2020-11-30 17:09:04,990 [DEBUG] kashgari - Loaded transformer model's vocab\n",
      "2020-11-30 17:09:04,991 [DEBUG] kashgari - config_path       : /home/Danny/pretrain_model/bert_tiny/bert_config.json\n",
      "2020-11-30 17:09:04,991 [DEBUG] kashgari - vocab_path      : /home/Danny/pretrain_model/bert_tiny/vocab.txt\n",
      "2020-11-30 17:09:04,992 [DEBUG] kashgari - checkpoint_path : /home/Danny/pretrain_model/bert_tiny/bert_model.ckpt\n",
      "2020-11-30 17:09:04,992 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]']\n",
      "2020-11-30 17:09:04,993 [DEBUG] kashgari - ------------------------------------------------\n",
      "Preparing text vocab dict: 100%|██████████| 931560/931560 [00:06<00:00, 138343.96it/s]\n",
      "Preparing text vocab dict: 100%|██████████| 232890/232890 [00:01<00:00, 121702.73it/s]\n",
      "2020-11-30 17:09:17,388 [DEBUG] kashgari - --- Build vocab dict finished, Total: 257059 ---\n",
      "2020-11-30 17:09:17,388 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '<LH>', 'the', 'to', 'I', 'a', 'and']\n",
      "Preparing classification label vocab dict: 100%|██████████| 931560/931560 [00:00<00:00, 1257174.59it/s]\n",
      "Preparing classification label vocab dict: 100%|██████████| 232890/232890 [00:00<00:00, 1294297.44it/s]\n",
      "2020-11-30 17:09:23,164 [DEBUG] kashgari - Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 128)    3906816     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 128)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    66048       Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    256         Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 128)    131712      Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Dropo (None, None, 128)    0           Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 128)    256         Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    256         Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 128)    131712      Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Dropo (None, None, 128)    0           Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 128)    256         Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 256)          198144      Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8)            2056        bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 8)            0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,569,608\n",
      "Trainable params: 200,200\n",
      "Non-trainable params: 4,369,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14555/14555 [==============================] - 311s 21ms/step - loss: 1.5893 - accuracy: 0.4153 - val_loss: 1.5107 - val_accuracy: 0.4439\n",
      "Epoch 2/5\n",
      "14555/14555 [==============================] - 316s 22ms/step - loss: 1.5302 - accuracy: 0.4376 - val_loss: 1.4865 - val_accuracy: 0.4551\n",
      "Epoch 3/5\n",
      "14555/14555 [==============================] - 306s 21ms/step - loss: 1.5121 - accuracy: 0.4447 - val_loss: 1.4791 - val_accuracy: 0.4584\n",
      "Epoch 4/5\n",
      "14555/14555 [==============================] - 303s 21ms/step - loss: 1.5035 - accuracy: 0.4481 - val_loss: 1.4731 - val_accuracy: 0.4593\n",
      "Epoch 5/5\n",
      "14555/14555 [==============================] - 301s 21ms/step - loss: 1.4984 - accuracy: 0.4499 - val_loss: 1.4679 - val_accuracy: 0.4612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18cdab1470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kashgari\n",
    "from kashgari.tasks.classification import BiGRU_Model\n",
    "kashgari.config.use_cudnn_cell = True\n",
    "import logging\n",
    "logging.basicConfig(level='DEBUG')\n",
    "from kashgari.embeddings import BertEmbedding\n",
    "bert_embed = BertEmbedding('/home/Danny/pretrain_model/bert_tiny')\n",
    "\n",
    "model = BiGRU_Model(bert_embed, sequence_length=128)\n",
    "model.fit(train_x, \n",
    "          train_y, \n",
    "          valid_x, \n",
    "          valid_y,\n",
    "          epochs=epochs,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-30 17:35:08,970 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 49\n",
      "2020-11-30 17:35:11,746 [DEBUG] kashgari - predict input shape (2, 291113, 49) x: \n",
      "(array([[ 101,  100, 1037, ...,    0,    0,    0],\n",
      "       [ 101,  100, 2256, ...,    0,    0,    0],\n",
      "       [ 101,  100, 2065, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101,  100, 2499, ...,    0,    0,    0],\n",
      "       [ 101,  100, 1998, ...,    0,    0,    0],\n",
      "       [ 101,  100,  100, ...,    0,    0,    0]], dtype=int32), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32))\n",
      "2020-11-30 17:36:23,654 [DEBUG] kashgari - predict output shape (291113, 8)\n",
      "2020-11-30 17:36:23,814 [DEBUG] kashgari - predict output argmax: [0 2 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger     0.8379    0.1067    0.1893      7946\n",
      "anticipation     0.5791    0.3603    0.4442     49984\n",
      "     disgust     0.4318    0.1287    0.1983     27669\n",
      "        fear     0.4027    0.1235    0.1891     12846\n",
      "         joy     0.4482    0.8647    0.5904    102943\n",
      "     sadness     0.3625    0.3436    0.3528     38745\n",
      "    surprise     0.8611    0.1137    0.2009      9816\n",
      "       trust     0.6290    0.1559    0.2499     41164\n",
      "\n",
      "    accuracy                         0.4598    291113\n",
      "   macro avg     0.5691    0.2746    0.3018    291113\n",
      "weighted avg     0.5058    0.4598    0.4065    291113\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-30 17:36:32,014 [INFO] kashgari - model saved to /home/Danny/Data-Mining/lab2/kaggle/kashgari/model/BiLSTM_BERT_epoch_5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/Danny/Data-Mining/lab2/kaggle/kashgari/model/BiLSTM_BERT_epoch_5'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)\n",
    "model_path = 'model/{}_epoch_{}'.format(model_name, epochs)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Danny/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The 'load_model' function is deprecated, use 'XX_Model.load_model' instead\n",
      "  \n",
      "2020-11-30 17:36:32,160 [DEBUG] kashgari - ------------------------------------------------\n",
      "2020-11-30 17:36:32,162 [DEBUG] kashgari - Loaded transformer model's vocab\n",
      "2020-11-30 17:36:32,162 [DEBUG] kashgari - config_path       : /home/Danny/pretrain_model/bert_tiny/bert_config.json\n",
      "2020-11-30 17:36:32,163 [DEBUG] kashgari - vocab_path      : /home/Danny/pretrain_model/bert_tiny/vocab.txt\n",
      "2020-11-30 17:36:32,163 [DEBUG] kashgari - checkpoint_path : /home/Danny/pretrain_model/bert_tiny/bert_model.ckpt\n",
      "2020-11-30 17:36:32,164 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]']\n",
      "2020-11-30 17:36:32,164 [DEBUG] kashgari - ------------------------------------------------\n",
      "2020-11-30 17:36:34,256 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 49\n",
      "2020-11-30 17:36:41,658 [DEBUG] kashgari - predict input shape (2, 291113, 49) x: \n",
      "(array([[ 101,  100, 1037, ...,    0,    0,    0],\n",
      "       [ 101,  100, 2256, ...,    0,    0,    0],\n",
      "       [ 101,  100, 2065, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101,  100, 2499, ...,    0,    0,    0],\n",
      "       [ 101,  100, 1998, ...,    0,    0,    0],\n",
      "       [ 101,  100,  100, ...,    0,    0,    0]], dtype=int32), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32))\n",
      "2020-11-30 17:37:47,126 [DEBUG] kashgari - predict output shape (291113, 8)\n",
      "2020-11-30 17:37:47,305 [DEBUG] kashgari - predict output argmax: [0 2 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger     0.8379    0.1067    0.1893      7946\n",
      "anticipation     0.5791    0.3603    0.4442     49984\n",
      "     disgust     0.4318    0.1287    0.1983     27669\n",
      "        fear     0.4027    0.1235    0.1891     12846\n",
      "         joy     0.4482    0.8647    0.5904    102943\n",
      "     sadness     0.3625    0.3436    0.3528     38745\n",
      "    surprise     0.8611    0.1137    0.2009      9816\n",
      "       trust     0.6290    0.1559    0.2499     41164\n",
      "\n",
      "    accuracy                         0.4598    291113\n",
      "   macro avg     0.5691    0.2746    0.3018    291113\n",
      "weighted avg     0.5058    0.4598    0.4065    291113\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'detail': {'anger': {'precision': 0.8379446640316206,\n",
       "   'recall': 0.10672036244651396,\n",
       "   'f1-score': 0.1893279749944184,\n",
       "   'support': 7946},\n",
       "  'anticipation': {'precision': 0.5791149417894127,\n",
       "   'recall': 0.36025528169014087,\n",
       "   'f1-score': 0.44418954586940973,\n",
       "   'support': 49984},\n",
       "  'disgust': {'precision': 0.4318292091217855,\n",
       "   'recall': 0.12866384762730854,\n",
       "   'f1-score': 0.19825689861610005,\n",
       "   'support': 27669},\n",
       "  'fear': {'precision': 0.4026896726719107,\n",
       "   'recall': 0.12354040168145726,\n",
       "   'f1-score': 0.18907487937094178,\n",
       "   'support': 12846},\n",
       "  'joy': {'precision': 0.44819564659144123,\n",
       "   'recall': 0.8646823970546808,\n",
       "   'f1-score': 0.590377587499088,\n",
       "   'support': 102943},\n",
       "  'sadness': {'precision': 0.3625197450841549,\n",
       "   'recall': 0.34355400696864113,\n",
       "   'f1-score': 0.3527821581437261,\n",
       "   'support': 38745},\n",
       "  'surprise': {'precision': 0.8611111111111112,\n",
       "   'recall': 0.1136919315403423,\n",
       "   'f1-score': 0.20086393088552915,\n",
       "   'support': 9816},\n",
       "  'trust': {'precision': 0.6290053895149437,\n",
       "   'recall': 0.15593722670294433,\n",
       "   'f1-score': 0.24991726527672334,\n",
       "   'support': 41164},\n",
       "  'accuracy': 0.45982487899887675,\n",
       "  'macro avg': {'precision': 0.5690512974895475,\n",
       "   'recall': 0.27463068196400364,\n",
       "   'f1-score': 0.30184878008199206,\n",
       "   'support': 291113},\n",
       "  'weighted avg': {'precision': 0.5058362215501865,\n",
       "   'recall': 0.45982487899887675,\n",
       "   'f1-score': 0.4064547297402413,\n",
       "   'support': 291113}},\n",
       " 'precision': 0.5058362215501865,\n",
       " 'recall': 0.45982487899887675,\n",
       " 'f1-score': 0.4064547297402413,\n",
       " 'support': 291113}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'model/{}_epoch_{}'.format(model_name, epochs)\n",
    "model = kashgari.utils.load_model(model_path)\n",
    "# model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.merge(text_df, left_on='tweet_id', right_on='tweet_id')\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = test_df['text'].tolist()\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-82c83de52d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredict_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/kashgari/tasks/classification/abc_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_data, batch_size, truncating, multi_label_threshold, predict_kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                                    \u001b[0msegment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                                                    \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                                                    max_position=self.embedding.max_position)\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'predict input shape {np.array(tensor).shape} x: \\n{tensor}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/kashgari/processors/sequence_processor.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, samples, seq_length, max_position, segment)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_bos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab2idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_bos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_eos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pad\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "predict_list = model.predict(text_list)\n",
    "predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predict'] = predict_list\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = test_df[['tweet_id', 'predict']]\n",
    "output_df = output_df.rename(columns={'tweet_id':'id', 'predict':'emotion'})\n",
    "# output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/{}_epoch_{}.csv'.format(model.name, epochs)\n",
    "output_df.to_csv(output_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
