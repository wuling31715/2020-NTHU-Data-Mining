{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# È†êË®≠ÊÉÖÊ≥Å‰∏ãÔºåtensorflowÊúÉÁÇ∫‰∫ÜÈÅøÂÖçÁ¢éÁâáÂåñÁöÑË®òÊÜ∂È´îÈÄ†ÊàêÊïàËÉΩ‰∏ç‰Ω≥ÁöÑÊÉÖÊ≥ÅËÄå‰∏ÄÊ¨°ÊÄßÁöÑ‰ΩîÁî®È°ØÂç°ÊâÄÊúâË®òÊÜ∂È´îÔºå\n",
    "# ÈÄôÈÉ®‰ªΩÂèØ‰ª•ÈÄèÈÅétf.config‰æÜÈôêÂà∂Á°¨È´îË≥áÊ∫ê‰ª•ÂèäÊåáÂÆöÈ°ØÂç°Ë®òÊÜ∂È´îÁöÑ‰ΩøÁî®„ÄÇ\n",
    "def select_gpu(N):\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    print(gpus)\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                # If memory growth is enabled for a PhysicalDevice, \n",
    "                # the runtime initialization will not allocate all memory on the device. \n",
    "                # Memory growth cannot be configured on a PhysicalDevice with virtual devices configured.\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # ÊåáÂÆö‰ΩøÁî®Âì™È°ÜGPU\n",
    "            tf.config.experimental.set_visible_devices(gpus[N], 'GPU')\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]\n",
      "3 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "select_gpu(0) \n",
    "epochs = 1\n",
    "model_name = 'bert-tiny'\n",
    "sequence_length = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/Danny/Data-Mining/lab2/kaggle/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = list()\n",
    "with open(data_path + 'tweets_DM.json' , 'r') as file:\n",
    "    for line in file:\n",
    "        json_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = list()\n",
    "for json in json_list:\n",
    "    tweet_id = json['_source']['tweet']['tweet_id']\n",
    "    hashtags = json['_source']['tweet']['hashtags']\n",
    "    hashtag = ' '.join(hashtags)\n",
    "    text = json['_source']['tweet']['text']\n",
    "#     text = text + ' ' + hashtag\n",
    "    tweet_list.append([tweet_id, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x38dba0</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x300ea2</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x360b99</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x22eecf</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x2fb282</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id       emotion\n",
       "0        0x3140b1       sadness\n",
       "1        0x368b73       disgust\n",
       "2        0x296183  anticipation\n",
       "3        0x2bd6e1           joy\n",
       "4        0x2ee1dd  anticipation\n",
       "...           ...           ...\n",
       "1455558  0x38dba0           joy\n",
       "1455559  0x300ea2           joy\n",
       "1455560  0x360b99          fear\n",
       "1455561  0x22eecf           joy\n",
       "1455562  0x2fb282  anticipation\n",
       "\n",
       "[1455563 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = pd.read_csv(data_path + 'emotion.csv')\n",
    "emotion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification\n",
       "0        0x28cc61           test\n",
       "1        0x29e452          train\n",
       "2        0x2b3819          train\n",
       "3        0x2db41f           test\n",
       "4        0x2a2acc          train\n",
       "...           ...            ...\n",
       "1867530  0x227e25          train\n",
       "1867531  0x293813          train\n",
       "1867532  0x1e1a7e          train\n",
       "1867533  0x2156a5          train\n",
       "1867534  0x2bb9d2          train\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identification_df = pd.read_csv(data_path + 'data_identification.csv')\n",
    "identification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "    \"üòÇ\": \"lolface\",\n",
    "    \"üòá\": \"smile\",\n",
    "    \"üòÄ\": \"smile\",\n",
    "    \"üéâ\": \"party\",\n",
    "    \"üò≥\": \"embarrassed\",\n",
    "    \"üòî\": \"sadface\",\n",
    "    \"üëÄ\": \"shifty\",\n",
    "    \"ü§∑\": \"shrugging\",\n",
    "    \"üíî\": \"brokenhearted\",\n",
    "    \"üëª\": \"ghost\",\n",
    "    \"üòç\": \"heart\",\n",
    "    \"üôÑ\": \"disdain\",\n",
    "    \"üíñ\": \"heart\",\n",
    "    \"‚úå\": \"victory\",\n",
    "    \"üé∂\": \"music\",\n",
    "    \"üò±\": \"shock\",\n",
    "    \"üòÉ\": \"smile\",\n",
    "    \"üòí\": \"unsatisfied\",\n",
    "    \"üëä\": \"brofist\",\n",
    "    \"üòÑ\": \"smile\",\n",
    "    \"üåû\": \"smile\",\n",
    "    \"üôå\": \"celebration\",\n",
    "    \"üòÅ\": \"smile\",\n",
    "    \"ü§ó\": \"hugging\",\n",
    "    \"ü§£\": \"rofl\",\n",
    "    \"üåà\": \"gaypride\",\n",
    "    \"üòâ\": \"winking\",\n",
    "    \"üíû\": \"heart\",\n",
    "    \"üôÉ\": \"irony\",\n",
    "    \"üòú\": \"winking\",\n",
    "    \"üò≠\": \"bawling\",\n",
    "    \"ü§î\": \"thinker\",\n",
    "    \"üòé\": \"cool\",\n",
    "    \"üíõ\": \"heart\",\n",
    "    \"üíö\": \"heart\",\n",
    "    \"üíÉ\": \"fun\",\n",
    "    \"üíó\": \"heart\",\n",
    "    \"üò¨\": \"awkward\",\n",
    "    \"üòå\": \"relieved\",\n",
    "    \"üòÖ\": \"whew\",\n",
    "    \"üíã\": \"kiss\",\n",
    "    \"üôà\": \"laugh\",\n",
    "    \"üòä\": \"^^\",\n",
    "    \"üëå\": \"okay\",\n",
    "    \"üò°\": \"angry\",\n",
    "    \"üòò\": \"kiss\",\n",
    "    \"üò©\": \"weary\",\n",
    "    \"üî•\": \"excellent\",\n",
    "    \"üíô\": \"heart\",\n",
    "    \"üíï\": \"heart\",\n",
    "    \"üëè\": \"clapping\",\n",
    "    \"üëç\": \"thumbsup\",\n",
    "    \"üíØ\": \"perfect\",\n",
    "    \"üíú\": \"heart\",\n",
    "    \"üïò\" : \"late\",\n",
    "    \"üò°\" : \"angry\",\n",
    "    \"üòí\" : \"dissatisfied\",\n",
    "    \"üò§\" : \"angry\",\n",
    "    \"üò†\" : \"angry\",\n",
    "    \"üòë\" : \"annoy\",\n",
    "    \"üò∞\": \"anxious\",\n",
    "    \"üòØ\": \"surprise\",\n",
    "    \"üò®\": \"scared\",\n",
    "    \"üò≤\": \"astonished\",\n",
    "    \"üí™\": \"strong\",\n",
    "    \"ü§¶\": \"facepalm\",\n",
    "    \"‚ú®\": \"sparkle\",\n",
    "    \"üò¢\": \"crying\",\n",
    "    \"üíì\": \"heart\",\n",
    "    \"üëë\": \"crown\",\n",
    "    \"ü§ò\": \"rockon\",\n",
    "    \"üåπ\": \"rose\",\n",
    "    \"üòã\": \"delicious\",\n",
    "    \"üòè\": \"flirting\",\n",
    "    \"üòÜ\": \"XD\",\n",
    "    \"üò´\": \"exhausted\",\n",
    "    \"üò¶\": \"frowning\",\n",
    "    \"üôè\": \"please\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_name_dict = {\n",
    "    \"#realdonaldtrump\": \"sadness\",\n",
    "    \"#fifthharmony\": \"sadness\",\n",
    "    \"#mostrequestlive\": \"sadness\",\n",
    "    \"#onairromeo\": \"sadness\",\n",
    "    \"#matthardybrand\": \"sadness\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...\n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n",
       "2        0x28b412  Confident of your obedience, I write to you, k...\n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>\n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...\n",
       "...           ...                                                ...\n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...\n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...\n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...\n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...\n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame(tweet_list, columns=['tweet_id', 'text'])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def replace_word(text):\n",
    "    text_list = text.split()\n",
    "    for i, j in enumerate(text_list):\n",
    "        if j in emoji_dict:\n",
    "            text_list[i] = emoji_dict[j]\n",
    "        if j in frequent_name_dict:\n",
    "            text_list[i] = frequent_name_dict[j]\n",
    "    text = ' '.join(text_list)\n",
    "    text = re.sub('<lh>|<|>|@|#|', '', text)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import nltk\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "# text_df['token'] = text_df['text'].apply(lambda s : s.lower())\n",
    "# text_df['token'] = text_df['token'].apply(lambda s : tweet_tokenizer.tokenize(s))\n",
    "# text_df['token'] = text_df['token'].apply(lambda s : ' '.join(s))\n",
    "# text_df['token'] = text_df['token'].apply(lambda s : replace_word(s))\n",
    "# text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from kashgari.tokenizers import BertTokenizer\n",
    "# vocab_path = '/home/Danny/pretrain_model/{}/vocab.txt'.format(model_name)\n",
    "# tokenizer = BertTokenizer.load_from_vocab_file(vocab_path)\n",
    "# text_df['token'] = text_df['text'].apply(lambda s : tokenizer.tokenize(s))\n",
    "# text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[People, who, post, \"add, me, on, #Snapchat\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[@brianklaas, As, we, see,, Trump, is, dangero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[Confident, of, your, obedience,, I, write, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[Now, ISSA, is, stalking, Tasha, üòÇüòÇüòÇ, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[\"Trust, is, not, the, same, as, faith., A, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[When, you, buy, the, last, 2, tickets, remain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[I, swear, all, this, hard, work, gone, pay, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[@Parcel2Go, no, card, left, when, I, wasn't, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[Ah,, corporate, life,, where, you, can, date,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Blessed, to, be, living, #Sundayvibes, &lt;LH&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "...           ...                                                ...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                                                     token  \n",
       "0        [People, who, post, \"add, me, on, #Snapchat\", ...  \n",
       "1        [@brianklaas, As, we, see,, Trump, is, dangero...  \n",
       "2        [Confident, of, your, obedience,, I, write, to...  \n",
       "3              [Now, ISSA, is, stalking, Tasha, üòÇüòÇüòÇ, <LH>]  \n",
       "4        [\"Trust, is, not, the, same, as, faith., A, fr...  \n",
       "...                                                    ...  \n",
       "1867530  [When, you, buy, the, last, 2, tickets, remain...  \n",
       "1867531  [I, swear, all, this, hard, work, gone, pay, o...  \n",
       "1867532  [@Parcel2Go, no, card, left, when, I, wasn't, ...  \n",
       "1867533  [Ah,, corporate, life,, where, you, can, date,...  \n",
       "1867534      [Blessed, to, be, living, #Sundayvibes, <LH>]  \n",
       "\n",
       "[1867535 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['token'] = text_df['text'].apply(lambda s : s.split())\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = identification_df[identification_df['identification'] == 'test']\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = text_df.merge(emotion_df, left_on='tweet_id', right_on='tweet_id')\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455563\n",
      "1455563\n"
     ]
    }
   ],
   "source": [
    "x_list = train_df['token'].to_list()\n",
    "y_list = train_df['emotion'].to_list()\n",
    "print(len(x_list))\n",
    "print(len(y_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931560 931560\n",
      "291113 291113\n",
      "232890 232890\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_list, y_list, test_size=0.2, random_state=42)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "print(len(train_x), len(train_y))\n",
    "print(len(test_x), len(test_y))\n",
    "print(len(valid_x), len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 13:51:33,296 [DEBUG] kashgari - ------------------------------------------------\n",
      "2020-12-04 13:51:33,298 [DEBUG] kashgari - Loaded transformer model's vocab\n",
      "2020-12-04 13:51:33,298 [DEBUG] kashgari - config_path       : /home/Danny/pretrain_model/bert-tiny/bert_config.json\n",
      "2020-12-04 13:51:33,303 [DEBUG] kashgari - vocab_path      : /home/Danny/pretrain_model/bert-tiny/vocab.txt\n",
      "2020-12-04 13:51:33,307 [DEBUG] kashgari - checkpoint_path : /home/Danny/pretrain_model/bert-tiny/bert_model.ckpt\n",
      "2020-12-04 13:51:33,307 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]']\n",
      "2020-12-04 13:51:33,308 [DEBUG] kashgari - ------------------------------------------------\n",
      "Preparing text vocab dict: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 931560/931560 [00:09<00:00, 102997.86it/s]\n",
      "Preparing text vocab dict: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232890/232890 [00:02<00:00, 103405.48it/s]\n",
      "2020-12-04 13:51:48,609 [DEBUG] kashgari - --- Build vocab dict finished, Total: 219591 ---\n",
      "2020-12-04 13:51:48,611 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '<LH>', 'the', 'to', 'I', 'a', 'and']\n",
      "Preparing classification label vocab dict: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 931560/931560 [00:00<00:00, 983559.66it/s] \n",
      "Preparing classification label vocab dict: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232890/232890 [00:00<00:00, 979623.96it/s] \n",
      "2020-12-04 13:51:53,233 [DEBUG] kashgari - Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 128)    3906816     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 128)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    66048       Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    256         Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 128)    131712      Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Dropo (None, None, 128)    0           Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 128)    256         Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    256         Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 128)    131712      Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Dropo (None, None, 128)    0           Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 128)    256         Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 256)          263168      Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8)            2056        bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 8)            0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,634,632\n",
      "Trainable params: 265,224\n",
      "Non-trainable params: 4,369,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3638/3638 [==============================] - 167s 46ms/step - loss: 1.6252 - accuracy: 0.4023 - val_loss: 1.5613 - val_accuracy: 0.4242\n",
      "CPU times: user 5min 11s, sys: 22.5 s, total: 5min 33s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import kashgari\n",
    "from kashgari.tasks.classification import BiLSTM_Model\n",
    "kashgari.config.use_cudnn_cell = True\n",
    "import logging\n",
    "logging.basicConfig(level='DEBUG')\n",
    "from kashgari.embeddings import BertEmbedding\n",
    "bert_embed = BertEmbedding('/home/Danny/pretrain_model/{}'.format(model_name))\n",
    "model = BiLSTM_Model(bert_embed, \n",
    "                     sequence_length=sequence_length,\n",
    "#                      sequence_length: Union[str, int] = 'auto',\n",
    "                    )\n",
    "history = model.fit(train_x, \n",
    "                    train_y, \n",
    "                    valid_x, \n",
    "                    valid_y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 13:54:50,120 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 45\n",
      "2020-12-04 13:54:59,954 [DEBUG] kashgari - predict input shape (2, 291113, 45) x: \n",
      "(array([[ 101,  100, 1037, ...,    0,    0,    0],\n",
      "       [ 101,  100, 2256, ...,    0,    0,    0],\n",
      "       [ 101,  100, 2065, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101,  100, 2499, ...,    0,    0,    0],\n",
      "       [ 101,  100, 1998, ...,    0,    0,    0],\n",
      "       [ 101,  100,  100, ...,    0,    0,    0]], dtype=int32), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)\n",
    "model_path = 'model/{}_epoch_{}'.format(model_name, epochs)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy', 'loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'model/{}_epoch_{}'.format(model_name, epochs)\n",
    "# model = kashgari.utils.load_model(model_path)\n",
    "# model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.merge(text_df, left_on='tweet_id', right_on='tweet_id')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = test_df['token'].tolist()\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = model.predict(text_list)\n",
    "# predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predict'] = predict_list\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = test_df[['tweet_id', 'predict']]\n",
    "output_df = output_df.rename(columns={'tweet_id':'id', 'predict':'emotion'})\n",
    "# output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/{}_epoch_{}.csv'.format(model_name, epochs)\n",
    "output_df.to_csv(output_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
