{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "model_name = 'bert-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/Danny/Data-Mining/lab2/kaggle/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = list()\n",
    "with open(data_path + 'tweets_DM.json' , 'r') as file:\n",
    "    for line in file:\n",
    "        json_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = list()\n",
    "for json in json_list:\n",
    "    tweet_id = json['_source']['tweet']['tweet_id']\n",
    "    hashtags = json['_source']['tweet']['hashtags']\n",
    "    hashtag = ' '.join(hashtags)\n",
    "    text = json['_source']['tweet']['text']\n",
    "    text = text + ' ' + hashtag\n",
    "    tweet_list.append([tweet_id, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x38dba0</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x300ea2</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x360b99</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x22eecf</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x2fb282</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id       emotion\n",
       "0        0x3140b1       sadness\n",
       "1        0x368b73       disgust\n",
       "2        0x296183  anticipation\n",
       "3        0x2bd6e1           joy\n",
       "4        0x2ee1dd  anticipation\n",
       "...           ...           ...\n",
       "1455558  0x38dba0           joy\n",
       "1455559  0x300ea2           joy\n",
       "1455560  0x360b99          fear\n",
       "1455561  0x22eecf           joy\n",
       "1455562  0x2fb282  anticipation\n",
       "\n",
       "[1455563 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = pd.read_csv(data_path + 'emotion.csv')\n",
    "emotion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification\n",
       "0        0x28cc61           test\n",
       "1        0x29e452          train\n",
       "2        0x2b3819          train\n",
       "3        0x2db41f           test\n",
       "4        0x2a2acc          train\n",
       "...           ...            ...\n",
       "1867530  0x227e25          train\n",
       "1867531  0x293813          train\n",
       "1867532  0x1e1a7e          train\n",
       "1867533  0x2156a5          train\n",
       "1867534  0x2bb9d2          train\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identification_df = pd.read_csv(data_path + 'data_identification.csv')\n",
    "identification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "    \"😂\": \"lolface\",\n",
    "    \"😇\": \"smile\",\n",
    "    \"😀\": \"smile\",\n",
    "    \"🎉\": \"party\",\n",
    "    \"😳\": \"embarrassed\",\n",
    "    \"😔\": \"sadface\",\n",
    "    \"👀\": \"shifty\",\n",
    "    \"🤷\": \"shrugging\",\n",
    "    \"💔\": \"brokenhearted\",\n",
    "    \"👻\": \"ghost\",\n",
    "    \"😍\": \"heart\",\n",
    "    \"🙄\": \"disdain\",\n",
    "    \"💖\": \"heart\",\n",
    "    \"✌\": \"victory\",\n",
    "    \"🎶\": \"music\",\n",
    "    \"😱\": \"shock\",\n",
    "    \"😃\": \"smile\",\n",
    "    \"😒\": \"unsatisfied\",\n",
    "    \"👊\": \"brofist\",\n",
    "    \"😄\": \"smile\",\n",
    "    \"🌞\": \"smile\",\n",
    "    \"🙌\": \"celebration\",\n",
    "    \"😁\": \"smile\",\n",
    "    \"🤗\": \"hugging\",\n",
    "    \"🤣\": \"rofl\",\n",
    "    \"🌈\": \"gaypride\",\n",
    "    \"😉\": \"winking\",\n",
    "    \"💞\": \"heart\",\n",
    "    \"🙃\": \"irony\",\n",
    "    \"😜\": \"winking\",\n",
    "    \"😭\": \"bawling\",\n",
    "    \"🤔\": \"thinker\",\n",
    "    \"😎\": \"cool\",\n",
    "    \"💛\": \"heart\",\n",
    "    \"💚\": \"heart\",\n",
    "    \"💃\": \"fun\",\n",
    "    \"💗\": \"heart\",\n",
    "    \"😬\": \"awkward\",\n",
    "    \"😌\": \"relieved\",\n",
    "    \"😅\": \"whew\",\n",
    "    \"💋\": \"kiss\",\n",
    "    \"🙈\": \"laugh\",\n",
    "    \"😊\": \"^^\",\n",
    "    \"👌\": \"okay\",\n",
    "    \"😡\": \"angry\",\n",
    "    \"😘\": \"kiss\",\n",
    "    \"😩\": \"weary\",\n",
    "    \"🔥\": \"excellent\",\n",
    "    \"💙\": \"heart\",\n",
    "    \"💕\": \"heart\",\n",
    "    \"👏\": \"clapping\",\n",
    "    \"👍\": \"thumbsup\",\n",
    "    \"💯\": \"perfect\",\n",
    "    \"💜\": \"heart\",\n",
    "    \"🕘\" : \"late\",\n",
    "    \"😡\" : \"angry\",\n",
    "    \"😒\" : \"dissatisfied\",\n",
    "    \"😤\" : \"angry\",\n",
    "    \"😠\" : \"angry\",\n",
    "    \"😑\" : \"annoy\",\n",
    "    \"😰\": \"anxious\",\n",
    "    \"😯\": \"surprise\",\n",
    "    \"😨\": \"scared\",\n",
    "    \"😲\": \"astonished\",\n",
    "    \"💪\": \"strong\",\n",
    "    \"🤦\": \"facepalm\",\n",
    "    \"✨\": \"sparkle\",\n",
    "    \"😢\": \"crying\",\n",
    "    \"💓\": \"heart\",\n",
    "    \"👑\": \"crown\",\n",
    "    \"🤘\": \"rockon\",\n",
    "    \"🌹\": \"rose\",\n",
    "    \"😋\": \"delicious\",\n",
    "    \"😏\": \"flirting\",\n",
    "    \"😆\": \"XD\",\n",
    "    \"😫\": \"exhausted\",\n",
    "    \"😦\": \"frowning\",\n",
    "    \"🙏\": \"please\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_name_dict = {\n",
    "    \"#realdonaldtrump\": \"sadness\",\n",
    "    \"#fifthharmony\": \"sadness\",\n",
    "    \"#mostrequestlive\": \"sadness\",\n",
    "    \"#onairromeo\": \"sadness\",\n",
    "    \"#matthardybrand\": \"sadness\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt; Sundayv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...\n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...\n",
       "2        0x28b412  Confident of your obedience, I write to you, k...\n",
       "3        0x1cd5b0               Now ISSA is stalking Tasha 😂😂😂 <LH> \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...\n",
       "...           ...                                                ...\n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...\n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...\n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...\n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...\n",
       "1867534  0x34be8c  Blessed to be living #Sundayvibes <LH> Sundayv...\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame(tweet_list, columns=['tweet_id', 'text'])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def replace_word(text):\n",
    "    text_list = text.split()\n",
    "    for i, j in enumerate(text_list):\n",
    "        if j in emoji_dict:\n",
    "            text_list[i] = emoji_dict[j]\n",
    "        if j in frequent_name_dict:\n",
    "            text_list[i] = frequent_name_dict[j]\n",
    "    text = ' '.join(text_list)\n",
    "    text = re.sub('<lh>|<|>|@|#|', '', text)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 34s, sys: 8.04 s, total: 2min 42s\n",
      "Wall time: 2min 43s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[people, who, post, \", add, me, on, snapchat, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[as, we, see, ,, trump, is, dangerous, to, fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[confident, of, your, obedience, ,, i, write, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>[now, issa, is, stalking, tasha, lolface, lolf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[\", trust, is, not, the, same, as, faith, ., a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[when, you, buy, the, last, 2, tickets, remain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[i, swear, all, this, hard, work, gone, pay, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[no, card, left, when, i, wasn't, in, so, i, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[ah, ,, corporate, life, ,, where, you, can, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt; Sundayv...</td>\n",
       "      <td>[blessed, to, be, living, sundayvibes, sundayv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3        0x1cd5b0               Now ISSA is stalking Tasha 😂😂😂 <LH>    \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "...           ...                                                ...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c  Blessed to be living #Sundayvibes <LH> Sundayv...   \n",
       "\n",
       "                                                     token  \n",
       "0        [people, who, post, \", add, me, on, snapchat, ...  \n",
       "1        [as, we, see, ,, trump, is, dangerous, to, fre...  \n",
       "2        [confident, of, your, obedience, ,, i, write, ...  \n",
       "3        [now, issa, is, stalking, tasha, lolface, lolf...  \n",
       "4        [\", trust, is, not, the, same, as, faith, ., a...  \n",
       "...                                                    ...  \n",
       "1867530  [when, you, buy, the, last, 2, tickets, remain...  \n",
       "1867531  [i, swear, all, this, hard, work, gone, pay, o...  \n",
       "1867532  [no, card, left, when, i, wasn't, in, so, i, h...  \n",
       "1867533  [ah, ,, corporate, life, ,, where, you, can, d...  \n",
       "1867534  [blessed, to, be, living, sundayvibes, sundayv...  \n",
       "\n",
       "[1867535 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "text_df['token'] = text_df['text'].apply(lambda s : s.lower())\n",
    "text_df['token'] = text_df['token'].apply(lambda s : tweet_tokenizer.tokenize(s))\n",
    "text_df['token'] = text_df['token'].apply(lambda s : ' '.join(s))\n",
    "text_df['token'] = text_df['token'].apply(lambda s : replace_word(s))\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = identification_df[identification_df['identification'] == 'test']\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = text_df.merge(emotion_df, left_on='tweet_id', right_on='tweet_id')\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455563\n",
      "1455563\n"
     ]
    }
   ],
   "source": [
    "x_list = train_df['token'].to_list()\n",
    "y_list = train_df['emotion'].to_list()\n",
    "print(len(x_list))\n",
    "print(len(y_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931560 931560\n",
      "291113 291113\n",
      "232890 232890\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_list, y_list, test_size=0.2, random_state=42)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "print(len(train_x), len(train_y))\n",
    "print(len(test_x), len(test_y))\n",
    "print(len(valid_x), len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 22:49:18,263 [DEBUG] kashgari - ------------------------------------------------\n",
      "2020-12-01 22:49:18,264 [DEBUG] kashgari - Loaded transformer model's vocab\n",
      "2020-12-01 22:49:18,264 [DEBUG] kashgari - config_path       : /home/Danny/pretrain_model/bert-base/bert_config.json\n",
      "2020-12-01 22:49:18,264 [DEBUG] kashgari - vocab_path      : /home/Danny/pretrain_model/bert-base/vocab.txt\n",
      "2020-12-01 22:49:18,265 [DEBUG] kashgari - checkpoint_path : /home/Danny/pretrain_model/bert-base/bert_model.ckpt\n",
      "2020-12-01 22:49:18,265 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]']\n",
      "2020-12-01 22:49:18,265 [DEBUG] kashgari - ------------------------------------------------\n",
      "Preparing text vocab dict: 100%|██████████| 931560/931560 [00:04<00:00, 232872.25it/s]\n",
      "Preparing text vocab dict: 100%|██████████| 232890/232890 [00:01<00:00, 227828.22it/s]\n",
      "2020-12-01 22:49:23,877 [DEBUG] kashgari - --- Build vocab dict finished, Total: 125642 ---\n",
      "2020-12-01 22:49:23,877 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '.', 'the', '!', 'to', ',', 'i']\n",
      "Preparing classification label vocab dict: 100%|██████████| 931560/931560 [00:00<00:00, 1850382.67it/s]\n",
      "Preparing classification label vocab dict: 100%|██████████| 232890/232890 [00:00<00:00, 1863720.76it/s]\n",
      "Calculating sequence length: 100%|██████████| 931560/931560 [00:00<00:00, 1937444.29it/s]\n",
      "Calculating sequence length: 100%|██████████| 232890/232890 [00:00<00:00, 1833754.08it/s]\n",
      "2020-12-01 22:49:25,707 [DEBUG] kashgari - Calculated sequence length = 29\n",
      "2020-12-01 22:49:26,829 [DEBUG] kashgari - Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "layer_embedding (Embedding)  (None, None, 100)         12564200  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 2056      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 12,800,752\n",
      "Trainable params: 12,800,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14555/14555 [==============================] - 1123s 77ms/step - loss: 1.2232 - accuracy: 0.5580 - val_loss: 1.1531 - val_accuracy: 0.5845\n",
      "Epoch 2/50\n",
      "14555/14555 [==============================] - 1103s 76ms/step - loss: 1.0343 - accuracy: 0.6281 - val_loss: 1.1721 - val_accuracy: 0.5869\n",
      "Epoch 3/50\n",
      "14555/14555 [==============================] - 1107s 76ms/step - loss: 0.8828 - accuracy: 0.6860 - val_loss: 1.3026 - val_accuracy: 0.5728\n",
      "Epoch 4/50\n",
      "14555/14555 [==============================] - 1196s 82ms/step - loss: 0.7458 - accuracy: 0.7367 - val_loss: 1.5023 - val_accuracy: 0.5573\n",
      "Epoch 5/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.6391 - accuracy: 0.7757 - val_loss: 1.7104 - val_accuracy: 0.5476\n",
      "Epoch 6/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.5593 - accuracy: 0.8040 - val_loss: 1.9376 - val_accuracy: 0.5379\n",
      "Epoch 7/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.4966 - accuracy: 0.8263 - val_loss: 2.1481 - val_accuracy: 0.5318\n",
      "Epoch 8/50\n",
      "14555/14555 [==============================] - 1205s 83ms/step - loss: 0.4475 - accuracy: 0.8438 - val_loss: 2.3132 - val_accuracy: 0.5261\n",
      "Epoch 9/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.4084 - accuracy: 0.8575 - val_loss: 2.4846 - val_accuracy: 0.5189\n",
      "Epoch 10/50\n",
      "14555/14555 [==============================] - 1205s 83ms/step - loss: 0.3767 - accuracy: 0.8685 - val_loss: 2.6176 - val_accuracy: 0.5227\n",
      "Epoch 11/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.3510 - accuracy: 0.8776 - val_loss: 2.7178 - val_accuracy: 0.5185\n",
      "Epoch 12/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.3292 - accuracy: 0.8853 - val_loss: 2.8479 - val_accuracy: 0.5160\n",
      "Epoch 13/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.3113 - accuracy: 0.8912 - val_loss: 2.9507 - val_accuracy: 0.5128\n",
      "Epoch 14/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.2970 - accuracy: 0.8962 - val_loss: 3.0408 - val_accuracy: 0.5153\n",
      "Epoch 15/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.2835 - accuracy: 0.9010 - val_loss: 3.1260 - val_accuracy: 0.5145\n",
      "Epoch 16/50\n",
      "14555/14555 [==============================] - 1204s 83ms/step - loss: 0.2732 - accuracy: 0.9042 - val_loss: 3.1904 - val_accuracy: 0.5143\n",
      "Epoch 17/50\n",
      "14555/14555 [==============================] - 1177s 81ms/step - loss: 0.2630 - accuracy: 0.9080 - val_loss: 3.2429 - val_accuracy: 0.5119\n",
      "Epoch 18/50\n",
      "14555/14555 [==============================] - 1158s 80ms/step - loss: 0.2540 - accuracy: 0.9108 - val_loss: 3.3470 - val_accuracy: 0.5130\n",
      "Epoch 19/50\n",
      "14555/14555 [==============================] - 1131s 78ms/step - loss: 0.2470 - accuracy: 0.9136 - val_loss: 3.3883 - val_accuracy: 0.5113\n",
      "Epoch 20/50\n",
      "14555/14555 [==============================] - 1128s 77ms/step - loss: 0.2401 - accuracy: 0.9156 - val_loss: 3.4764 - val_accuracy: 0.5098\n",
      "Epoch 21/50\n",
      "14555/14555 [==============================] - 1128s 77ms/step - loss: 0.2349 - accuracy: 0.9177 - val_loss: 3.4916 - val_accuracy: 0.5087\n",
      "Epoch 22/50\n",
      "14555/14555 [==============================] - 1128s 77ms/step - loss: 0.2292 - accuracy: 0.9196 - val_loss: 3.5373 - val_accuracy: 0.5083\n",
      "Epoch 23/50\n",
      "14555/14555 [==============================] - 1128s 77ms/step - loss: 0.2245 - accuracy: 0.9214 - val_loss: 3.5707 - val_accuracy: 0.5087\n",
      "Epoch 24/50\n",
      "14555/14555 [==============================] - 1124s 77ms/step - loss: 0.2209 - accuracy: 0.9224 - val_loss: 3.6411 - val_accuracy: 0.5123\n",
      "Epoch 25/50\n",
      "14555/14555 [==============================] - 1123s 77ms/step - loss: 0.2174 - accuracy: 0.9239 - val_loss: 3.6370 - val_accuracy: 0.5069\n",
      "Epoch 26/50\n",
      "14555/14555 [==============================] - 1123s 77ms/step - loss: 0.2139 - accuracy: 0.9250 - val_loss: 3.6751 - val_accuracy: 0.5067\n",
      "Epoch 27/50\n",
      "14555/14555 [==============================] - 1123s 77ms/step - loss: 0.2108 - accuracy: 0.9259 - val_loss: 3.6944 - val_accuracy: 0.5078\n",
      "Epoch 28/50\n",
      "14555/14555 [==============================] - 1123s 77ms/step - loss: 0.2079 - accuracy: 0.9266 - val_loss: 3.7595 - val_accuracy: 0.5086\n",
      "Epoch 29/50\n",
      "14555/14555 [==============================] - 1123s 77ms/step - loss: 0.2057 - accuracy: 0.9276 - val_loss: 3.7705 - val_accuracy: 0.5088\n",
      "Epoch 30/50\n",
      "14555/14555 [==============================] - 1122s 77ms/step - loss: 0.2014 - accuracy: 0.9290 - val_loss: 3.8220 - val_accuracy: 0.5091\n",
      "Epoch 31/50\n",
      "14555/14555 [==============================] - 1123s 77ms/step - loss: 0.2002 - accuracy: 0.9295 - val_loss: 3.8094 - val_accuracy: 0.5093\n",
      "Epoch 32/50\n",
      "14555/14555 [==============================] - 1124s 77ms/step - loss: 0.1978 - accuracy: 0.9302 - val_loss: 3.8424 - val_accuracy: 0.5090\n",
      "Epoch 33/50\n",
      "14555/14555 [==============================] - 1156s 79ms/step - loss: 0.1955 - accuracy: 0.9311 - val_loss: 3.8723 - val_accuracy: 0.5083\n",
      "Epoch 34/50\n",
      "14555/14555 [==============================] - 1134s 78ms/step - loss: 0.1924 - accuracy: 0.9321 - val_loss: 3.9061 - val_accuracy: 0.5068\n",
      "Epoch 35/50\n",
      " 2379/14555 [===>..........................] - ETA: 15:43 - loss: 0.1900 - accuracy: 0.9328"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import kashgari\n",
    "from kashgari.tasks.classification import BiLSTM_Model\n",
    "kashgari.config.use_cudnn_cell = True\n",
    "import logging\n",
    "logging.basicConfig(level='DEBUG')\n",
    "from kashgari.embeddings import BertEmbedding\n",
    "bert_embed = BertEmbedding('/home/Danny/pretrain_model/{}'.format(model_name))\n",
    "model = BiLSTM_Model(bert_embed, sequence_length='auto')\n",
    "model = BiLSTM_Model()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "history = model.fit(train_x, \n",
    "                    train_y, \n",
    "                    valid_x, \n",
    "                    valid_y,\n",
    "                    epochs=epochs,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_x, test_y)\n",
    "model_path = 'model/{}_epoch_{}'.format(model_name, epochs)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy', 'loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'model/{}_epoch_{}'.format(model_name, epochs)\n",
    "# model = kashgari.utils.load_model(model_path)\n",
    "# model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.merge(text_df, left_on='tweet_id', right_on='tweet_id')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = test_df['token'].tolist()\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = model.predict(text_list)\n",
    "predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predict'] = predict_list\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = test_df[['tweet_id', 'predict']]\n",
    "output_df = output_df.rename(columns={'tweet_id':'id', 'predict':'emotion'})\n",
    "# output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/{}_epoch_{}.csv'.format(model_name, epochs)\n",
    "output_df.to_csv(output_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
